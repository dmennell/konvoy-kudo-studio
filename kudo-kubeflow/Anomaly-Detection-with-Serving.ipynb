{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have each Python cell auto-formatted\n",
    "# See: https://black.readthedocs.io\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kfp\n",
      "Version: 0.5.0\n",
      "Summary: KubeFlow Pipelines SDK\n",
      "Home-page: UNKNOWN\n",
      "Author: google\n",
      "Author-email: None\n",
      "License: UNKNOWN\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: tabulate, click, kfp-server-api, strip-hints, PyYAML, google-auth, google-cloud-storage, requests-toolbelt, Deprecated, argo-models, cloudpickle, jsonschema, kubernetes\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "import kfp.components as components\n",
    "import kfp.dsl as dsl\n",
    "import kubeflow.fairing.utils\n",
    "from kfp.components import InputPath, OutputPath, InputTextFile, OutputTextFile\n",
    "from kfp.components import func_to_container_op\n",
    "\n",
    "NAMESPACE = kubeflow.fairing.utils.get_current_k8s_namespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function arguments specified with `InputPath` and `OutputPath` are the key to defining dependencies.\n",
    "For now, it suffices to think of them as the input and output of each step.\n",
    "How we can define dependencies is explained in the [next section](#How-to-Combine-the-Components-into-a-Pipeline).\n",
    "\n",
    "### Component 1: Download & Prepare the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting minio_secret.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile minio_secret.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: minio-s3-secret\n",
    "  annotations:\n",
    "     serving.kubeflow.org/s3-endpoint: minio-service.kubeflow:9000\n",
    "     serving.kubeflow.org/s3-usehttps: \"0\" # Default: 1. Must be 0 when testing with MinIO!\n",
    "type: Opaque\n",
    "data:\n",
    "  awsAccessKeyID: bWluaW8=\n",
    "  awsSecretAccessKey: bWluaW8xMjM=\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: default\n",
    "secrets:\n",
    "  - name: minio-s3-secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/minio-s3-secret unchanged\n",
      "serviceaccount/default configured\n"
     ]
    }
   ],
   "source": [
    "! kubectl apply -f minio_secret.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_process_dataset(data_dir: OutputPath(str)):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    # Generated training sequences for use in the model.\n",
    "    def create_sequences(values, time_steps=288):\n",
    "        output = []\n",
    "        for i in range(len(values) - time_steps):\n",
    "            output.append(values[i : (i + time_steps)])\n",
    "        return np.stack(output)\n",
    "\n",
    "    master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n",
    "\n",
    "    df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n",
    "    df_small_noise_url = master_url_root + df_small_noise_url_suffix\n",
    "    df_small_noise = pd.read_csv(\n",
    "        df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n",
    "    )\n",
    "\n",
    "    df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n",
    "    df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n",
    "    df_daily_jumpsup = pd.read_csv(\n",
    "        df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n",
    "    )\n",
    "\n",
    "    # Normalize and save the mean and std we get,\n",
    "    # for normalizing test data.\n",
    "    training_mean = df_small_noise.mean()\n",
    "    training_std = df_small_noise.std()\n",
    "    df_training_value = (df_small_noise - training_mean) / training_std\n",
    "    print(\"Number of training samples:\", len(df_training_value))\n",
    "\n",
    "    x_train = create_sequences(df_training_value.values)\n",
    "    print(\"Training input shape: \", x_train.shape)\n",
    "    np.save(data_dir + \"/x_train.npy\", x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(data_dir: InputPath(str), model_dir: OutputPath(str)):\n",
    "\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from tensorflow import keras\n",
    "    from keras import layers\n",
    "    import tensorflow as tf\n",
    "\n",
    "    x_train = np.load(data_dir + \"/x_train.npy\")\n",
    "\n",
    "    print(\"Training input shape: \", x_train.shape)\n",
    "\n",
    "    #     TODO: make x_train into a layer so it can be used as input layer below\n",
    "\n",
    "    class Conv1DTranspose(tf.keras.layers.Layer):\n",
    "        def __init__(self, filters, kernel_size, strides=1, padding=\"valid\"):\n",
    "            super().__init__()\n",
    "            self.conv2dtranspose = tf.keras.layers.Conv2DTranspose(\n",
    "                filters, (kernel_size, 1), (strides, 1), padding\n",
    "            )\n",
    "\n",
    "        def call(self, x):\n",
    "            x = tf.expand_dims(x, axis=2)\n",
    "            x = self.conv2dtranspose(x)\n",
    "            x = tf.squeeze(x, axis=2)\n",
    "            return x\n",
    "\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "            layers.Conv1D(\n",
    "                filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "            ),\n",
    "            layers.Dropout(rate=0.2),\n",
    "            layers.Conv1D(\n",
    "                filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"\n",
    "            ),\n",
    "            Conv1DTranspose(filters=16, kernel_size=7, padding=\"same\", strides=2),\n",
    "            layers.Dropout(rate=0.2),\n",
    "            Conv1DTranspose(filters=32, kernel_size=7, padding=\"same\", strides=2),\n",
    "            Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        x_train,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model.save(model_dir)\n",
    "    print(f\"Model saved {model_dir}\")\n",
    "    print(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine into pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_serve(\n",
    "    data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mesosphere/kubeflow:1.0.1-0.3.1-tensorflow-2.2.0\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_and_process_dataset, base_image=BASE_IMAGE\n",
    "    )()\n",
    "\n",
    "    trainOp = components.func_to_container_op(\n",
    "        build_and_train_model, base_image=BASE_IMAGE\n",
    "    )(downloadOp.output)\n",
    "\n",
    "    #     evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n",
    "    #         downloadOp.output, trainOp.output\n",
    "    #     )\n",
    "\n",
    "    #     exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "    #         trainOp.output, evaluateOp.output, export_bucket, model_name, model_version\n",
    "    #     )\n",
    "\n",
    "    #     # Create an inference server from an external component\n",
    "    #     kfserving_op = components.load_component_from_url(\n",
    "    #         \"https://raw.githubusercontent.com/kubeflow/pipelines/8d738ea7ddc350e9b78719910982abcd8885f93f/components/kubeflow/kfserving/component.yaml\"\n",
    "    #     )\n",
    "    #     kfserving = kfserving_op(\n",
    "    #         action=\"create\",\n",
    "    #         default_model_uri=f\"s3://{export_bucket}/{model_name}\",\n",
    "    #         model_name=\"mnist\",\n",
    "    #         namespace=NAMESPACE,\n",
    "    #         framework=\"tensorflow\",\n",
    "    #     )\n",
    "\n",
    "    #     kfserving.after(exportOp)\n",
    "\n",
    "    # See: https://github.com/kubeflow/kfserving/blob/master/docs/DEVELOPER_GUIDE.md#troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/a7b2a150-7e83-4ff5-bf8d-758124da00ad\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/6df6b433-a869-4888-958f-f8b4e148ac42\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=\"anomaly-pipeline\")\n",
    "def anomaly_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    export_bucket: str = \"airframe-anomaly\",\n",
    "    model_name: str = \"airframe-anomaly\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    train_and_serve(\n",
    "        data_dir=data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)\n",
    "\n",
    "\n",
    "pipeline_func = anomaly_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"airframe-anomaly-detection\"\n",
    "\n",
    "arguments = {\n",
    "    \"model_dir\": \"/train/model\",\n",
    "    \"data_dir\": \"/train/data\",\n",
    "    #     \"export_bucket\": \"airframe-anomaly\",\n",
    "    \"model_name\": \"airframe-anomaly\",\n",
    "    \"model_version\": \"1\",\n",
    "}\n",
    "\n",
    "client = kfp.Client()\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    pipeline_func,\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=run_name,\n",
    "    arguments=arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 3: Evaluate the Model\n",
    "With the following Python function the model is evaluated.\n",
    "The metrics [metadata](https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/) (loss and accuracy) is available to the Kubeflow Pipelines UI.\n",
    "Metadata can automatically be visualized with output viewer(s).\n",
    "Please go [here](https://www.kubeflow.org/docs/pipelines/sdk/output-viewer/) to see how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    data_dir: InputPath(str), model_dir: InputPath(str), metrics_path: OutputPath(str)\n",
    ") -> NamedTuple(\"EvaluationOutput\", [(\"mlpipeline_metrics\", \"Metrics\")]):\n",
    "    \"\"\"Loads a saved model from file and uses a pre-downloaded dataset for evaluation.\n",
    "    Model metrics are persisted to `/mlpipeline-metrics.json` for Kubeflow Pipelines\n",
    "    metadata.\"\"\"\n",
    "\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "    from collections import namedtuple\n",
    "\n",
    "    def normalize_test(values, mean, std):\n",
    "        values -= mean\n",
    "        values /= std\n",
    "        return values\n",
    "\n",
    "\n",
    "    df_test_value = (df_daily_jumpsup - training_mean) / training_std\n",
    "\n",
    "    # Create sequences from test values.\n",
    "    x_test = create_sequences(df_test_value.values)\n",
    "    print(\"Test input shape: \", x_test.shape)\n",
    "\n",
    "    # Get test MAE loss.\n",
    "    x_test_pred = model.predict(x_test)\n",
    "    test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "    test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "\n",
    "    # Detect all the samples which are anomalies.\n",
    "    anomalies = test_mae_loss > threshold\n",
    "    print(\"Number of anomaly samples: \", np.sum(anomalies))\n",
    "    print(\"Indices of anomaly samples: \", np.where(anomalies))\n",
    "   \n",
    "\n",
    "    ds_test, ds_info = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=\"test\",\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        withj_info=True,\n",
    "        download=False,\n",
    "        data_dir=data_dir,\n",
    "    )\n",
    "\n",
    "    # See: https://www.tensorflow.org/datasets/keras_example#build_training_pipeline\n",
    "    ds_test = ds_test.map(\n",
    "        normalize_image, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    model = tf.keras.models.load_model(model_dir)\n",
    "    (loss, accuracy) = model.evaluate(ds_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(metrics_path, \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    out_tuple = namedtuple(\"EvaluationOutput\", [\"mlpipeline_metrics\"])\n",
    "\n",
    "    return out_tuple(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 4: Export the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(\n",
    "    model_dir: InputPath(str),\n",
    "    metrics: InputPath(str),\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    import os\n",
    "    import boto3\n",
    "    from botocore.client import Config\n",
    "\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://minio-service.kubeflow:9000\",\n",
    "        aws_access_key_id=\"minio\",\n",
    "        aws_secret_access_key=\"minio123\",\n",
    "        config=Config(signature_version=\"s3v4\"),\n",
    "    )\n",
    "\n",
    "    # Create export bucket if it does not yet exist\n",
    "    response = s3.list_buckets()\n",
    "    export_bucket_exists = False\n",
    "\n",
    "    for bucket in response[\"Buckets\"]:\n",
    "        if bucket[\"Name\"] == export_bucket:\n",
    "            export_bucket_exists = True\n",
    "\n",
    "    if not export_bucket_exists:\n",
    "        s3.create_bucket(ACL=\"public-read-write\", Bucket=export_bucket)\n",
    "\n",
    "    # Save model files to S3\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for filename in files:\n",
    "            local_path = os.path.join(root, filename)\n",
    "            s3_path = os.path.relpath(local_path, model_dir)\n",
    "\n",
    "            s3.upload_file(\n",
    "                local_path,\n",
    "                export_bucket,\n",
    "                f\"{model_name}/{model_version}/{s3_path}\",\n",
    "                ExtraArgs={\"ACL\": \"public-read\"},\n",
    "            )\n",
    "\n",
    "    response = s3.list_objects(Bucket=export_bucket)\n",
    "    print(f\"All objects in {export_bucket}:\")\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(\"{}/{}\".format(export_bucket, file[\"Key\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Combine the Components into a Pipeline\n",
    "Note that up to this point we have not yet used the Kubeflow Pipelines SDK!\n",
    "\n",
    "With our four components (i.e. self-contained funtions) defined, we can wire up the dependencies with Kubeflow Pipelines.\n",
    "\n",
    "The call [`components.func_to_container_op(f, base_image=img)(*args)`](https://www.kubeflow.org/docs/pipelines/sdk/sdk-overview/) has the following ingredients:\n",
    "- `f` is the Python function that defines a component\n",
    "- `img` is the base (Docker) image used to package the function\n",
    "- `*args` lists the arguments to `f`\n",
    "\n",
    "What the `*args` mean is best explained by going forward through the graph:\n",
    "- `downloadOp` is the very first step and has no dependencies; it therefore has no `InputPath`.\n",
    "  Its output (i.e. `OutputPath`) is stored in `data_dir`.\n",
    "- `trainOp` needs the data downloaded from `downloadOp` and its signature lists `data_dir` (input) and `model_dir` (output).\n",
    "  So, it _depends on_ `downloadOp.output` (i.e. the previous step's output) and stores its own outputs in `model_dir`, which can be used by another step.\n",
    "  `downloadOp` is the parent of `trainOp`, as required.\n",
    "- `evaluateOp`'s function takes three arguments: `data_dir` (i.e. `downloadOp.output`), `model_dir` (i.e. `trainOp.output`), and `metrics_path`, which is where the function stores its evaluation metrics.\n",
    "  That way, `evaluateOp` can only run after the successful completion of both `downloadOp` and `trainOp`.\n",
    "- `exportOp` runs the function `export_model`, which accepts five parameters: `model_dir`, `metrics`, `export_bucket`, `model_name`, and `model_version`.\n",
    "  From where do we get the `model_dir`?\n",
    "  It is nothing but `trainOp.output`.\n",
    "  Similarly, `metrics` is `evaluateOp.output`.\n",
    "  The remaining three arguments are regular Python arguments that are static for the pipeline: they do not depend on any step's output being available.\n",
    "  Hence, they are defined without using `InputPath`.\n",
    "  Since it is the last step of the pipeline, we also do not list any `OutputPath` for use in another step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_serve(\n",
    "    data_dir: str,\n",
    "    model_dir: str,\n",
    "    export_bucket: str,\n",
    "    model_name: str,\n",
    "    model_version: int,\n",
    "):\n",
    "    # For GPU support, please add the \"-gpu\" suffix to the base image\n",
    "    BASE_IMAGE = \"mesosphere/kubeflow:1.0.1-0.3.1-tensorflow-2.2.0\"\n",
    "\n",
    "    downloadOp = components.func_to_container_op(\n",
    "        download_dataset, base_image=BASE_IMAGE\n",
    "    )()\n",
    "\n",
    "    trainOp = components.func_to_container_op(train_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output\n",
    "    )\n",
    "\n",
    "    evaluateOp = components.func_to_container_op(evaluate_model, base_image=BASE_IMAGE)(\n",
    "        downloadOp.output, trainOp.output\n",
    "    )\n",
    "\n",
    "    exportOp = components.func_to_container_op(export_model, base_image=BASE_IMAGE)(\n",
    "        trainOp.output, evaluateOp.output, export_bucket, model_name, model_version\n",
    "    )\n",
    "\n",
    "    # Create an inference server from an external component\n",
    "    kfserving_op = components.load_component_from_url(\n",
    "        \"https://raw.githubusercontent.com/kubeflow/pipelines/8d738ea7ddc350e9b78719910982abcd8885f93f/components/kubeflow/kfserving/component.yaml\"\n",
    "    )\n",
    "    kfserving = kfserving_op(\n",
    "        action=\"create\",\n",
    "        default_model_uri=f\"s3://{export_bucket}/{model_name}\",\n",
    "        model_name=\"mnist\",\n",
    "        namespace=NAMESPACE,\n",
    "        framework=\"tensorflow\",\n",
    "    )\n",
    "\n",
    "    kfserving.after(exportOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case it isn't obvious: this will build the Docker images for you.\n",
    "Each image is based on `BASE_IMAGE` and includes the Python functions as executable files.\n",
    "Each component _can_ use a different base image though.\n",
    "This may come in handy if you want to have reusable components for automatic data and/or model analysis (e.g. to investigate bias).\n",
    "\n",
    "Note that you did not have to use [Kubeflow Fairing](../fairing/Kubeflow%20Fairing.ipynb) or `docker build` locally at all!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    Remember when we said all dependencies have to be included in the base image?\n",
    "    Well, that was not quite accurate.\n",
    "    It's a good idea to have everything included and tested before you define and use your pipeline components to make sure that there are not dependency conflicts.\n",
    "    There is, however, a way to add <a href=\"https://kubeflow-pipelines.readthedocs.io/en/latest/source/kfp.components.html#kfp.components.func_to_container_op\">packages (<code>packages_to_install</code>) and additional code to execute <em>before</em> the function code (<code>extra_code</code>)</a>.\n",
    "</div>\n",
    "\n",
    "Is that it?\n",
    "Not quite!\n",
    "\n",
    "We still have to define the pipeline itself.\n",
    "Our `train_and_serve` function defines dependencies but we must use the KFP domain-specific language (DSL) to register the pipeline with its four components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://github.com/kubeflow/kfserving/blob/master/docs/DEVELOPER_GUIDE.md#troubleshooting\n",
    "def op_transformer(op):\n",
    "    op.add_pod_annotation(name=\"sidecar.istio.io/inject\", value=\"false\")\n",
    "    return op\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"End-to-End MNIST Pipeline\",\n",
    "    description=\"A sample pipeline to demonstrate multi-step model training, evaluation, export, and serving\",\n",
    ")\n",
    "def mnist_pipeline(\n",
    "    model_dir: str = \"/train/model\",\n",
    "    data_dir: str = \"/train/data\",\n",
    "    export_bucket: str = \"mnist\",\n",
    "    model_name: str = \"mnist\",\n",
    "    model_version: int = 1,\n",
    "):\n",
    "    train_and_serve(\n",
    "        data_dir=data_dir,\n",
    "        model_dir=model_dir,\n",
    "        export_bucket=export_bucket,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "    )\n",
    "    dsl.get_pipeline_conf().add_op_transformer(op_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, let's submit the pipeline directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 200902 12:10:38 _client:267] Creating experiment End-to-End MNIST Pipeline.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/03de75c6-5b30-44cf-8eb2-335d41345ea7\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/e8f0d36e-737b-4d5a-bb76-bdb902501aa3\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline_func = mnist_pipeline\n",
    "run_name = pipeline_func.__name__ + \" run\"\n",
    "experiment_name = \"End-to-End MNIST Pipeline\"\n",
    "\n",
    "arguments = {\n",
    "    \"model_dir\": \"/train/model\",\n",
    "    \"data_dir\": \"/train/data\",\n",
    "    \"export_bucket\": \"mnist\",\n",
    "    \"model_name\": \"mnist\",\n",
    "    \"model_version\": \"1\",\n",
    "}\n",
    "\n",
    "client = kfp.Client()\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    pipeline_func,\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=run_name,\n",
    "    arguments=arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph will look like this:\n",
    "\n",
    "![Graph](./img/graph.png)\n",
    "\n",
    "If there are any issues with our pipeline definition, this is where they would flare up.\n",
    "So, until you submit it, you won't know if your pipeline definition is correct.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    We have so far claimed that Kubeflow Pipelines is for automation of multi-step (ad hoc) workflows and usage in CI/CD.\n",
    "    You may have wondered why that is.\n",
    "    After all, it is possible to set up <a href=\"https://www.kubeflow.org/docs/pipelines/overview/concepts/run/\">recurring runs</a> of pipelines.\n",
    "    The reason is that these pipeline steps are one-offs.\n",
    "    Even though you can parameterize each step, including the ones that kick off an entire pipeline, there is no orchestration of workflows.\n",
    "    Stated differently, if a step fails, there is no mechanism for automatic retries.\n",
    "    Nor is there any support for marking success: if the step is scheduled to be run again, it will be run again, whether or not the previous execution was successful, obviating any subsequent runs (except in cases where it may be warranted).\n",
    "    Kubeflow Pipelines allows <a href=\"https://www.kubeflow.org/docs/pipelines/reference/api/kubeflow-pipeline-api-spec/#operation--apis-v1beta1-runs--run_id--retry-post\">retries</a> but it is not configurable out of the box.\n",
    "    If you want Airflow- or Luigi-like behaviour for dependency management of workflows, Kubeflow Pipelines is not the tool.\n",
    "</div>\n",
    "\n",
    "## How to Predict with the Inference Server\n",
    "The simplest way to check that our inference server is up and running is to check it with `curl` ( pre-installed on the cluster).\n",
    "\n",
    "To do so, let's define a few helper functions for plotting and displaying images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def display_image(x_test, image_index):\n",
    "    plt.imshow(x_test[image_index].reshape(28, 28), cmap=\"binary\")\n",
    "\n",
    "\n",
    "def predict_number(model, x_test, image_index):\n",
    "    pred = model.predict(x_test[image_index : image_index + 1])\n",
    "    print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_test = x_test / 255.0  # We must transform the data in the same way as before!\n",
    "\n",
    "image_index = 1005\n",
    "\n",
    "display_image(x_test, image_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference server expects a JSON payload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json\n",
    "\n",
    "tf_serving_req = {\"instances\": x_test[image_index : image_index + 1].tolist()}\n",
    "\n",
    "with open(\"input.json\", \"w\") as json_file:\n",
    "    json.dump(tf_serving_req, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"predictions\": [[1.40836079e-07, 5.1750153e-06, 2.76334504e-06, 0.00259963516, 0.00124566338, 4.93358129e-06, 1.92605313e-07, 0.000128401283, 0.00166672678, 0.994346321]\n",
      "    ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "model = \"mnist\"\n",
    "url = f\"http://{model}-predictor-default.{NAMESPACE}.svc.cluster.local/v1/models/{model}:predict\"\n",
    "\n",
    "! curl -L $url -d@input.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probabilities for each class (0-9) are shown in the `predictions` response.\n",
    "The model believes the image shows a \"9\", which indeed it does!\n",
    "\n",
    "For more details on the URL, please check out this [example](https://github.com/kubeflow/kfserving/tree/master/docs/samples/tensorflow#run-a-prediction)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
