)

# """
# ## Prepare training data

# Get data values from the training timeseries data file and normalize the
# `value` data. We have a `value` for every 5 mins for 14 days.

# -   24 * 60 / 5 = **288 timesteps per day**
# -   288 * 14 = **4032 data points** in total
# """


# # Normalize and save the mean and std we get,
# # for normalizing test data.
# training_mean = df_small_noise.mean()
# training_std = df_small_noise.std()
# df_training_value = (df_small_noise - training_mean) / training_std
# print("Number of training samples:", len(df_training_value))

# """
# ### Create sequences
# Create sequences combining `TIME_STEPS` contiguous data values from the
# training data.
# """

# TIME_STEPS = 288

# # Generated training sequences for use in the model.
# def create_sequences(values, time_steps=TIME_STEPS):
#     output = []
#     for i in range(len(values) - time_steps):
#         output.append(values[i : (i + time_steps)])
#     return np.stack(output)


# x_train = create_sequences(df_training_value.values)
# print("Training input shape: ", x_train.shape)

# """
# ## Build a model

# We will build a convolutional reconstruction autoencoder model. The model will
# take input of shape `(batch_size, sequence_length, num_features)` and return
# output of the same shape. In this case, `sequence_length` is 288 and
# `num_features` is 1.
# """

# model = keras.Sequential(
#     [
#         layers.Input(shape=(x_train.shape[1], x_train.shape[2])),
#         layers.Conv1D(
#             filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
#         ),
#         layers.Dropout(rate=0.2),
#         layers.Conv1D(
#             filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
#         ),
#         layers.Conv1DTranspose(
#             filters=16, kernel_size=7, padding="same", strides=2, activation="relu"
#         ),
#         layers.Dropout(rate=0.2),
#         layers.Conv1DTranspose(
#             filters=32, kernel_size=7, padding="same", strides=2, activation="relu"
#         ),
#         layers.Conv1DTranspose(filters=1, kernel_size=7, padding="same"),
#     ]
# )
# model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss="mse")
# print(model.summary())

# """
# ## Train the model

# Please note that we are using `x_train` as both the input and the target
# since this is a reconstruction model.
# """

# history = model.fit(
#     x_train,
#     x_train,
#     epochs=10,
#     batch_size=128,
#     validation_split=0.1,
#     callbacks=[
#         keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, mode="min")
#     ],
# )


# """
# ## Detecting anomalies

# We will detect anomalies by determining how well our model can reconstruct
# the input data.


# 1.   Find MAE loss on training samples.
# 2.   Find max MAE loss value. This is the worst our model has performed trying
# to reconstruct a sample. We will make this the `threshold` for anomaly
# detection.
# 3.   If the reconstruction loss for a sample is greater than this `threshold`
# value then we can infer that the model is seeing a pattern that it isn't
# familiar with. We will label this sample as an `anomaly`.


# """

# # Get train MAE loss.
# x_train_pred = model.predict(x_train)
# train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)

# # Get reconstruction loss threshold.
# threshold = np.max(train_mae_loss)
# print("Reconstruction error threshold: ", threshold)


# """
# ### Prepare test data
# """

# df_test_value = (df_daily_jumpsup - training_mean) / training_std

# # Create sequences from test values.
# x_test = create_sequences(df_test_value.values)
# print("Test input shape: ", x_test.shape)

# # Get test MAE loss.
# x_test_pred = model.predict(x_test)
# test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)
# test_mae_loss = test_mae_loss.reshape((-1))

# # Detect all the samples which are anomalies.
# anomalies = test_mae_loss > threshold
# print("Number of anomaly samples: ", np.sum(anomalies))
# print("Indices of anomaly samples: ", np.where(anomalies))